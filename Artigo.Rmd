---
title: "Regressão Quantílica"
author:
- Luiz Fernando Palin Droubi^[SPU/SC, lfpdroubi@gmail.com]
- Carlos Augusto Zilli^[IFSC, carloszilli@gmail.com]
- Murilo Damian Ribeiro^[UFSC, murilodamianr@gmail.com ]
- Norberto Hochheim^[UFSC, hochheim@gmail.com]
date: "`r format(Sys.Date(), '%d/%m/%Y')`"
output:
  pdf_document: 
    includes:
      in_header: preamble.tex
    keep_tex: yes
    number_sections: yes
    citation_package: biblatex
  word_document: default
  html_document:
    fig_caption: yes
    keep_md: yes
classoption: a4paper, 12pt
documentclass: article
geometry: left=3.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm
link-citations: yes
linkcolor: red
urlcolor: magenta
citecolor: blue
biblio-style: abnt
subtitle: Quando utilizar!?
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = FALSE, message=FALSE, warning = FALSE, 
                      fig.ext='png', fig.align='center', fig.path = "images/",
                      fig.pos = "H", dev = "png", dpi = 600, out.width = "70%")
type <- knitr::opts_knit$get("rmarkdown.pandoc.to")
options(digits = 10)
library(quantreg)
library(appraiseR)
library(ggplot2)
library(ggthemes)
theme_set(theme_stata())
library(stargazer)
library(mosaic)
library(LaplacesDemon)
library(truncnorm)
```

# Resumo {-}

A NBR 14.653-02 recomenda que, na Engenharia de Avaliações de imóveis urbanos, 
para o tratamento dos dados, seja utilizada metodologia científica, mesmo no
tratamento de dados por fatores, o que usualmente é feito através do método da
regressão linear, ainda que a norma também cite outros métodos, como a regressão 
espacial, a  análise envoltória de dados e as redes neurais artificiais.
No entanto, através destes métodos, o que se obtém são coeficientes ou fatores 
**médios** da contribuição de uma característica do imóvel na formação do valor 
final. Ocorre que a contribuição de uma determinada característica para a formação
do valor final dos imóveis pode ser diferente para os diferentes quantis da 
distribuição de probabilidades. É possível até que uma determinada característica 
que se mostre insignificante no método da regressão linear seja significativa na 
regressão quantílica, já que na regressão linear o que se estima é se, **em média**,
uma determinada característica tem influência na formação do valor total de um 
imóvel. Ocorre que uma determinada característica pode influenciar positivamente
o preço de venda dos imóveis de maior valor e negativamente o preço de venda dos 
imóveis de menor valor (ou *vice versa*), sendo que, **em média**, o seu efeito 
seja nulo, o que no entanto não quer dizer que aquela variável não tenha qualquer 
influência na formação de preço dos imóveis do mercado em análise. Em suma, esta 
diferente contribuição das características no valor final dos imóveis, atualmente, 
é ignorada, sendo apenas utilizado o valor médio, sendo que diferentes efeitos das 
características em imóveis de valores diferentes são negligenciadas. A regressão 
quantílica é um método que permite estimar a real influência de cada 
característica ao longo de toda a distribuição de probabilidades dos imóveis de 
um mercado, o que pode se demonstrar útil na avaliação de imóveis urbanos em
determinados mercados o que atualmente pode passar despercebido aos olhos do 
avaliador acostumado com os métodos estatísticos clássicos.

# Introdução

@Zietz mostra que os conflitos a respeito da contribuição de uma determinada 
característica na formação dos preços de venda de imóveis residenciais podem ser 
esclarecidos através da regressão quantílica. Diferentes valores para os 
coeficientes de regressão linear para algumas características podem ser encontrados 
ao longo da distribuição de preços de venda de imóveis. Ou seja, algumas 
características dos imóveis residenciais podem ser mais valorizados por compradores 
de imóveis de mais alto valor do que por compradores de imóveis de menor valor.

Segundo @Zietz, variáveis como área construída, área do lote e número de
banheiros tem um impacto maior nos imóveis de maior valor de venda, enquanto outras
variáveis parecem ter um comportamento constante para todos os preços de venda
de imóveis, como garagens e distância ao centro, entre outras.

A regressão quantílica permite que a influência de uma característica qualquer de
um imóvel tenha efeitos diferentes para diferentes faixas de valores de imóveis.

É possível perceber em alguns casos como o mostrado na figura \ref{fig:trutas},
que com a regressão linear pode-se chegar à conclusão de que não há qualquer 
correlação entre duas variáveis, já que, em média, o efeito do regressor é 
nulo ($\beta = 0$), o que pode ser visto na reta tracejada na figura, que é a reta 
de regressão linear.

No entanto, como bem observaram \textcite[412-413]{QReco}, se os autores somente 
tivessem se baseado na regressão linear, eles teriam erroneamente concluído que
não há relação alguma entre as variávies $X$ e $y$. Na realidade, a relação pode
ser percebida apenas nos quantis superiores dos dados. Fisicamente, o que deve 
ser deduzido é que, com o aumento do quociente largura/profundidade de um canal,
limita-se a densidade de trutas no canal. Esta limitação não é percebida nos 
quantis inferiores dos dados. Por isso, em média, o efeito é nulo, mas claramente
o efeito pode ser percebido nos quantis superiores.

```{r trutas, fig.cap="Mudanças na densidade de trutas ($y$) em função do quociente largura/profundidade de um canal ($X$). Fonte: \\textcite[413]{QReco}.", out.width="100%"}
knitr::include_graphics("images/-000.png")
```

O mesmo comportamento poderia ser encontrado na Engenharia de Avaliações. Imagine-se
que ao analisar um mercado de lotes urbanos o Engenheiro de Avaliações se depare
com os dados mostrados na figura \ref{fig:urb}. Ao analisar os dados através da
regressão linear à média (reta tracejada), o Engenheiro poderia, erroneamente
concluir que a área não teria nenhum efeito sobre o valor unitário dos lotes, 
quando na realidade o que ocorre é que o efeito da variável área é o de aumentar
suavemente o valor unitário dos lotes de menor valor, e diminuir de uma forma
um pouco mais agressiva o valor unitário dos lotes dos quantis superiores. 

Em suma, globalmente, o efeito médio é zero, o que não significa que a variável 
não possua qualquer significância na formação de valor dos imóveis.

```{r urb, fig.cap="Dados gerados randomicamente para ilustrar a importância da regressão quantílica na Engenharia de Avaliações. Fonte: do Autor.", out.width="100%"}
set.seed(5)
x = rtruncnorm(500, a = 125, b = 600, mean = 360, sd = 100)
b0 = 2500 # intercept chosen at your choice
b1 = 0 # coef chosen at your choice
h = function(x) 250 - .25*x # h performs heteroscedasticity function (here I used a linear one)
i = function(x) b0 + .25*x # lower bound limit
y = rtruncnorm(500, a = i(x), mean = b0, sd = h(x))
#y = b0 + b1*x + eps - h(x)
dados <- data.frame(VU = y, Area = x)
#X <- X[which(X[, "y"] > 2400), ]
fit <- lm(VU ~ Area, data = dados)
fit1 <- rq(VU ~ Area, tau = .10, data = dados)
fit2 <- rq(VU ~ Area, tau = .25, data = dados)
fit3 <- rq(VU ~ Area, tau = .50, data = dados)
fit4 <- rq(VU ~ Area, tau = .75, data = dados)
fit5 <- rq(VU ~ Area, tau = .90, data = dados)
fit6 <- rq(VU ~ Area, tau = .95, data = dados)
par(mar=c(5,5,2,6), bg = "light blue")
plot(VU ~ Area, data = dados, main = "A importância de analisar todos os quantis",
     xlab = expression("Área"~(m^2)),
     ylab = expression("VU (R$/"~m^2~")"))
abline(fit, lty = 2, lwd = 2)
abline(fit1, col = "yellow")
abline(fit2, col = "red")
abline(fit3, col = "green")
abline(fit4, col = "blue")
abline(fit5, col = "darkred")
abline(fit6, col = "darkblue")
legend('topleft', xpd = TRUE, inset=c(1,0), bty = "n",
       legend=c(".10", ".25", ".50", ".75", ".90", "0.95", "mean"),
       col = c("yellow", "red", "green", "blue", "darkred", "darkblue", "black"), 
       lty = c(rep(1, 6), 2),
       lwd = c(rep(1,6), 2),
       title= expression(~tau), text.font=4, bg='lightblue')
```


# Revisão Bibliográfica


## Breve Histórico

@stigler1986

Segundo @koenker2000 [p. 347], o gráfico mais importante da história da 
estatística é o gráfico de Galton, reproduzido na figura 1.

O gráfico ilustra o fenômeno, descoberto por Galton, da regressão à média, cuja
importância até hoje se faz presente em diversos estudos científicos. Em estudos
clínicos para a aferição do real efeito de um novo medicamento, por exemplo, há 
necessidade de se estabelecerem dois grupos de pacientes (chamados de grupos de 
controle e de grupo de tratamento) para isolar os efeitos do tratamento pesquisado 
do efeito do regressão à média (ou reversão à média) [ver @james1973]. Nestes
estudos, apenas as pessoas do grupo de tratamento realmente são tratadas com o 
mediamento em teste. Desta maneira, a diferença das médias entre os grupos é 
isolada da variação biológica natural e da variação devido aos erros de aferição,
que estão sujeitos à regressão à média.


![Regressão à média (Galton, 1885). Fonte: \textcite[348]{koenker2000}](image_Galton.png)

Segundo @koenker2000 [p. 349], a característica essencial da regressão linear 
clássica, derivada deste gráfico, é que o efeito do covariante na variável 
resposta é inteiramente capturado pela expressão abaixo, uma simples "mudança de 
local", enquanto a aleatoriedade remanescente de $Y$ dado $X$ é modelada por um 
termo de erro aditivo independente de $X$.

$$\mathbb{E}(Y|X = x) = x'\beta$$

Talvez prevendo que o seu método fizesse os seus colegas estatísticos se aterem
apenas ao estudo das médias,  @galton [p. 62] [*apud* @koenker2000, p. 350]
repreendeu seus colegas estatísticos que:

> limited their inquiries to Averages, and do not seem to revel in more 
> comprehensive views. Their souls seem as dull to the charm of variety as that 
> of a native of one of our flat English counties, whose retrospect of Switzerland
> was that, if the mountains could be thrown into its lakes, two nuisances would
> be got rid of at once.

Ironicamente, contudo, apesar da repreensão de Galton, muito provavelmente foi o
seu método o principal responsável por restringir o escopo das investigações 
estatísticas à comparação de médias por décadas [@koenker2000, 350].

Muito anterior ao descobrimento por @galton do fenômeno da regressão à média e
do método dos mínimos quadrados por @legendre1805 [^1], segundo @tortoise [p. 281], 
Boscovich propôs em 1760, o seguinte problema, onde buscava estimar a elipsidade 
da terra, prevista na obra-prima de Isaac Newton [-@newton],  em :

Encontrar $\hat \alpha$ e $\hat \beta$ tais que:

$$y_i = \hat \alpha + \hat \beta x_i + \hat u_i$$

com $\sum \hat u_i = 0$ e $\sum |\hat u_i| = \text{min!}$.

Laplace resolveu o problema matematicamente em 1789 [@tortoise, 281]. 

Posteriormente, com a chegada do *méthode des moindres carrés*, ou seja, do método 
dos mínimos quadrados ordinários, o método dos mínimos erros absolutos de Laplace 
ficou em segundo plano, até que Edgeworth, em 1887 propõe o primeiro algoritmo 
capaz de obter o intercepto e o coeficiente angular da reta do método 
dos mínimos desvios absolutos, relaxando a restrição de que a soma dos resíduos 
seja igual a zero ($\sum \hat u_i = 0$)  [@tortoise, 281].

Na década de 40 surgiram os primeiros algoritmos simplex destinados à otimização, 
algoritmos estes que se ajustam às necessidades do métodos dos mínimos desvios 
absolutos, que não possui solução analítica, mas iterativa. A primeira aplicação 
do método dos mínimos desvios absolutos se deve a Arrow e Hoffenberg, em 1959 [@tortoise, 281].

@koenker1978 generalizaram o problema de minimização do erro médio absoluto, o 
que equivale à regressão à mediana, ao problema de encontrar os diversos quantis 
de distribuição através da aplicação de uma função de perda assimétrica, 
correspondente àquele quantil, chegando-se assim à regressão quantílica.

[^1]: @gauss1809 ligou o método dos mínimos quadrados à distribuição normal
mas a origem do método se deve ao trabalho pioneiro de @legendre1805. Houve 
discordâncias entre os dois na disputa pela invenção do método, entre outros
achados na época. Ver a este respeito @STIGLER197731; @stigler1981 e @stigler1986.

## Referencial teórico

### Interpretação Geométrica

#### O gráfico dual

### Estimação de quantis

Existem diversas formas de se obter os quantis de uma amostra.

### O problema de estimar quantis como um problema de minimização

Pode-se demonstrar que, assim como a média aritmética $\mu$ de uma variável aleatória 
tem a propriedade de minimizar a soma dos desvios quadráticos de cada observação 
em relação a ela [@matloff2017, 50], a mediana tem a propriedade de minimizar a 
soma dos desvios médios absolutos de cada observação [@matloff2017, 260]. Ou seja:

$$\mu(Y) = \mathbb{E}Y = \arg \min_c \sum_{i = 1}^n \frac{1}{n}(y_i - c)^2$$
$$Me = \arg \min_c \sum_{i = 1}^n \frac{1}{n}|y_i - c|$$
Sabe-se que a mediana de uma variável equivale ao quantil de 50%. Assim, outros
quantis podem ser obtidos com formulação análoga à formulação acima, porém com a 
aplicação de uma função de perda assimétrica ($\rho_\tau(.)$) em lugar da função 
módulo (ver figura 1):

$$Q_\tau(Y) = \arg \min_c \sum_{i = 1}^n \rho_\tau(y_i - c)$$

```{r, fig.cap= "Função de perda ou custo."}
knitr::include_graphics("DmKq7.png")
```

\bcenter Fonte: @qr.
\ecenter


### Regressão linear e quantílica

A regressão linear pode ser vista como uma forma de minimização, assim como a 
média de uma população pode ser visto como o problema de minimização descrito acima.

A diferença é que no caso da regressão linear, ao invés de minimizar em relação 
a um escalar, desta vez se minimiza o erro em prever uma variável Y em relação a ]
uma função de outra variável X, f(X). Pode-se demonstrar que entre todas as funções
f(X), a que minimiza o erro médio quadrático de Y dado X ($\mathbb{E}[(Y - f(X))^2]$)
é a função de regressão $\mu(t) = \mathbb{E}(Y|X=t)$ [@matloff2017, 49-50].

Analogamente, pode-se demonstrar que a mediana condicional é a função que minimiza
o erro médio absoluto de Y dado X ($\mathbb{E}(|Y-f(X)|)$) [@matloff2017, 260-261].

#### Unicidade da solução

Pode-se demonstrar que a regressão linear, ou seja, a minimização de $\mathbb{E}[(Y - X\beta)^2]$
possui uma única solução e esta solução pode ser encontrada analiticamente, bastando
para isso efetuar a derivação parcial deste termo e igualando-o a zero, obtendo-se
assum um único solução para o cálculo do valor de $\beta$ [ver @matloff2017, 49-50].

O mesmo não se pode dizer da regressão à mediana a mais genericamente da 
regressão quantílica. Nesta abordagem, há múltiplas soluções possíveis, assim
como numa amostra de tamanho par existem duas medianas possíveis. Ainda, as soluções
do problema de minimização da regressão quantílica não podem ser encontradas 
analiticamente, sendo necessária a utilização de precessos iterativos para a 
obtenção do(s) mínimo(s).

Contudo, deve-se ter em mente que, em ambos os processos de minimização, seja para 
a regressão linear ou para a regressão quantílica, trabalha-se com apenas uma amostra
da população estudada. Desta forma, os valores de $\hat \beta$ encontrados são
apenas estimativas dos valores reais de $\beta$, ou seja, os valores da população.

Assim, deve-se levar em conta que a diferença entre as múltiplas soluções da regressão
quantílica é da ordem de $1/n$, enquanto a amplitude da precisão da estimativa é
de tamanho $1/\sqrt{n}$. Assim, presume-se que as múltiplas soluções possíveis, para
os casos práticos estão dentro da margem de erro para a primeira estimativa encontrada
pelo algoritmo.

#### Robustez da solução

#### Transformação e retransformação

$$Q_{f(Y)}(\tau) = f(Q_Y(\tau))$$

#### Eficiência computacional

#### Estimador de máxima verossimilhança

Pode-se demonstrar que, quando a distribuição é normal o estimador de máxima 
verossimilhança para o parâmetro $\mu$ da distribuição é a média amostral.

Analogamente, se a distribuição dos dados for a distribuição de Laplace, o estimador
de máxima verossimilhança para o parâmetro é a mediana.

Isto implica que, se a distribuição dos dados é normal, são necessários $\pi/2$ 
(1,57) vezes mais dados para que a estimativa de $\mu$ através da mediana seja 
tão eficiente quanto a estimativa através da média. Isto implica, por sua vez, 
que intervalos de confiança obtidos para a regressão quantílica são 25% mais 
largos do que os intervalos de confiança para a regressão linear [@koenker2000, 354; @dasGupta, 92].

No entanto, se a distribuição dos dados for a distribuição de Laplace, pode-se
demonstrar que são necessários duas vezes mais dados para que a média estime
$\mu$ com a mesma precisão da mediana.

#### Média

```{r dist_normal, fig.cap = "Distribuição Normal."}
xcnorm(0, return = "plot", system = "gg") %>%
  gf_labs(title = "Distribuição Normal")
```

$$\hat \mu = \frac{1}{n}\sum x_i$$
$$\hat \sigma = \frac{1}{n-1} \sum_{i=1}^n (x_i - \hat \mu)^2$$
$$f(x|\mu, \sigma) = \frac{1}{\sigma\sqrt{2/\pi}}\exp \left (-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right )$$

#### Mediana

```{r dist_Laplace, fig.cap = "Distribuição de Laplace."}
#Plot Probability Functions
x <- seq(from=-5, to=5, by=0.1)
plot(x, dlaplace(x,0,0.5), ylim=c(0,1), type="l", main="Distribuição de Laplace",
     ylab="density", col="red")
lines(x, dlaplace(x,0,1), type="l", col="green")
lines(x, dlaplace(x,0,2), type="l", col="blue")
legend(2, 0.9, expression(paste(mu==0, ", ", lambda==0.5),
     paste(mu==0, ", ", lambda==1), paste(mu==0, ", ", lambda==2)),
     lty=c(1,1,1), col=c("red","green","blue"))
```

$$\hat \mu = \arg\min_c \sum |x_i - c|$$
$$\hat \lambda = \frac{1}{n} \sum_{i=1}^n |x_i - \hat \mu|$$

$$f(x|\mu, \lambda) = \frac{1}{2 \lambda} \exp \left ( -\frac{|x - \mu|}{\lambda}\right )$$

### Inferência

## Aplicações da regressão quantílica

### Na Engenharia de Avaliações

@Zietz mostra...


# Estudos de Caso

Para os estudos de caso foram utilizados os dados disponíveis em @hochheim.

```{r}
dados <- centro_2015@data
dados$padrao <- as.numeric(dados$padrao)
```

## Duas dimensões

Assim como na regressão linear, é mais fácil aa compreensão da regressão quantílica através de exemplos em duas dimensões, e depois generalizar para $n$ dimensões.

Seja primeiramente o caso de dados heteroscedásticos. A figura \ref{fig:qr1} ilustra a aplicação da regressão quantílica e da regressão linear para este caso. Na figura \ref{fig:qr1}, a reta vermelha é a reta de regressão linear entre as variáveis. A área sombreada em cinza é o intervalo de confiança para a regressão linear \@80\%. As retas azuis são as retas de regressão quantílica para os quantis 0,1; 0,2; 0,3; 0,4; 0,5; 0,6; 0,7; 0,8 e 0,9.

A regressão quantílica neste caso pode ser usada para demonstrar a não validade dos intervalos de confiança (IC) e predição (IP) para a regressão linear para este tipo de dados: como a variância da população não é constante, mas aumenta com o aumento da área, as retas da regressão quantílica se abrem. Como os intervalos de confiança e predição na inferência clássica são calculados considerando-se que a variância da população é constante, este efeito não se observa no formato do IC.

```{r qr1, fig.cap = "Regressão Linear e Quantílica para dados heteroscedásticos."}
qs <- 1:9/10
qr <- rq(valor ~ area_total, data = dados, tau = qs)
ggplot(dados, aes(area_total, valor)) + 
  geom_point() + 
  geom_quantile(quantiles = qs) + 
  geom_smooth(method="lm", se = TRUE, level = .80, color = "red")
```

Assim como na regressão linear, uma conveniente transformação das variáveis pode ser aplicada para a obtenção da homoscedasticidade. Isto pode ser visto na figura \ref{fig:qr2}, onde as retas para os diferentes quantis obtidas pela regressão quantílica agora são praticamente paralelas entre si, indicando que a heteroscedasticidade foi removida.

```{r qr2, fig.cap = "Regressão Linear e Quantílica com dados transformados."}
qs <- 1:9/10
qr2 <- rq(log(valor) ~ log(area_total), data = dados, tau = qs)
ggplot(dados, aes(log(area_total), log(valor))) + 
  geom_point() + 
  geom_quantile(quantiles = qs) + 
  geom_smooth(method="lm", se = FALSE, color = "red")
```

Os coeficientes das retas de regressão quantílica podem ser plotados como na figura \ref{fig:coef1}. Nesta figura, a reta cheia vermelha representa o coeficiente do modelo de regressão linear, enquanto a reta preta pontilhada representa os vários coeficientes da regressão quantílica. As retas vermelhas tracejadas representam o intervalo de confiança de estimação do coeficiente de regressão linear. A área sombreada em cinza representa os intervalos de confiança para os coeficientes da regressão quantílica. Deve-se notar que, entre os quantis aproximados de 0,3 e 0,55, os coeficientes da regressão quantílica não são significamente diferentes, estatísticamente, do coeficiente da regreessão linear.

```{r coef1, fig.cap = "Variação dos coeficientes de regressão quantílica (variáveis originais)."}
plot(summary(qr), parm="area_total")
```
Já para os dados transformados, pode-se notar na figura \ref{fig:coef2} que para todos os quantis, os coeficientes da regressão quantílica não podem ser considerados estatisticamente diferentes do coeficiente da regressão linear. Também se pode notar nesta figura como o estimador de regressão linear, para uma variável normalmente distribuída e na ausência de heteroscedasticidade, é mais eficiente do que o estimador da regressão quantílica, como a teoria já prevê (ver @matloff2017, 238). 

(Zilli, não sei se tu pesquisou isso na revisão bibliográfica, mas acho que se não, era bom colocar! Colocar algo do tipo: as vantagens e desvantagens da regressão quantílica. Apesar da regressão quantílica ser robusta à presença de *outliers*, ela é menos eficiente do que a regressao linear, caso a distribuição da variável estudada seja normal, claro.)

```{r coef2, fig.cap = "Variação dos coeficientes de regressão quantílica (variáveis transformadas)."}
plot(summary(qr2), parm="log(area_total)")
```


## Análise Multivariada

Para os dados obtidos de Hochheim [-@hochheim, 22-23] foram ajustados dois modelos, um de regressão linear, com os dados saneados, e outro de regressão quantílica, utilizando-se a totalidade dos dados, para os quantis 0,1; 0,2; 0,3; 0,4; 0,5; 0,6; 0,7; 0,8 e 0,9.

Na figura \ref{fig:coefs} podem ser vistos os valores dos coeficientes de cada variável para os diferentes quantis. Pode-se perceber, mais uma vez, que o valor dos coeficientes da regressão quantílica não diferem significantemente dos coeficientes da regressão linear (exceção para alguns quantis superiores nas variáveis `area_total` e `padrao`).

```{r}
fit_qr <- rq(log(valor) ~ area_total + quartos + suites + garagens + 
            log(dist_b_mar) + rec(padrao), data = dados, tau = qs)
```


```{r coefs, out.width="100%", fig.cap="Coeficientes de regressão linear e quantílica. Análise multivariada."}
plot(summary(fit_qr))
```



Na tabela \ref{tab:fits} podem ser vistos os coeficientes e estatísticas básicas dos modelos de regressão linear e de regressão à mediana (quantil 0,5).

```{r}
fit <- lm(log(valor) ~ area_total + quartos + suites + garagens + 
            log(dist_b_mar) + rec(padrao), data = dados,
          subset = -c(31, 39))
fit_1 <- rq(log(valor) ~ area_total + quartos + suites + garagens + 
            log(dist_b_mar) + rec(padrao), data = dados)
# fit_2 <- rq(log(valor) ~ area_total + quartos + suites + garagens + 
#             log(dist_b_mar) + rec(padrao), data = dados, tau = c(.1, .9))
fit_2 <- rq(log(valor) ~ area_total + quartos + suites + garagens +
            log(dist_b_mar) + rec(padrao), data = dados, tau = .1)
fit_3 <- rq(log(valor) ~ area_total + quartos + suites + garagens +
            log(dist_b_mar) + rec(padrao), data = dados, tau = .9)
```



```{r, results='asis'}
stargazer(fit, fit_1, type = type, header = FALSE, label = "tab:fits",
          title = "Comparação entre os modelos de regressão linear e regressão à mediana.",
          report = "vcstp*", ci = TRUE, ci.level = .80, star.cutoffs = c(0.30, 0.20, 0.10))
```

```{r}
p <- predict(fit, newdata = dados[52,], interval = "confidence", level = .80)
pp <- predict(fit, newdata = dados[52,], interval = "prediction", level = .80)
p1 <- predict(fit_1, newdata = dados[52,], interval = "confidence", level = .80)
p2 <- predict(fit_2, newdata = dados[52,], interval = "confidence", level = .80)
p3 <- predict(fit_3, newdata = dados[52,], interval = "confidence", level = .80)
```

### Estimativas

É interessante comparar as estimativas obtidas com os modelos de regressão linear, com dados saneados, e o modelo de regressão à mediana, com a totalidade dos dados. Por um lado, o modelo de regressão linear tende a ser mais preciso para a estimação da média, como prevê a teoria. Por outro lado, com mais dados, o modelo de regressão à mediana pode tornar-se mais eficiente.

Deve-se levar em conta que as estimativas com o modelo de regressão linear aqui apresentadas são para a mediana da distribuição lognormal. 

Pelo modelo de regressão linear, o valor da estimativa central  encontrado foi de R\$`r brformat(exp(p[, "fit"]))`, com intervalo de confiança entre R\$ `r brformat(exp(p[, "lwr"]))` e R\$ `r brformat(exp(p[, "upr"]))`. A amplitude do intervalo de confiança foi de `r porcento(amplitude(exp(p))/100)`.

Já pelo modelo de regressão quantílica, o valor da estimativa central encontrado foi de R\$`r brformat(exp(p1[, "fit"]))`, com intervalo de confiança entre R\$ `r brformat(exp(p1[, "lower"]))` e R\$ `r brformat(exp(p1[, "higher"]))`. A amplitude do intervalo de confiança foi de  `r porcento(amplitude(exp(p1))/100)`.

O modelo de regressão linear mostrou-se, portanto, mais eficiente do que o modelo de regressão a mediana, apesar no menor número de dados.

Os limites inferior e superior do intervalo de predição \@80\% para o modelo de regressão linear são, respectivamente:  R\$ `r brformat(exp(pp[, "lwr"]))` e R\$ `r brformat(exp(pp[, "upr"]))`.

Para o modelo de regressão quantílica, o intervalo de predição não faz qualquer sentido. No entanto, é possível estimar os valores diretamente para os quantis 0,1 e 0,9 da população. Nesta caso, os valores encontrados foram, respectivamente: R\$ `r brformat(exp(p2[, "fit"]))` e R\$ `r brformat(exp(p3[, "fit"]))`.

Podem ainda ser calculados os intervalos de confiança \@80\% para as estimativas dos quantis 0,1 e 0,9. 

Os limites inferior e superior do IC para o quantil 0,1 são, respectivamente: R\$ `r brformat(exp(p2[, "lower"]))` e R\$ `r brformat(exp(p2[, "higher"]))`.

Os limites inferior e superior do IC para o quantil 0,9 são, respectivamente: R\$ `r brformat(exp(p3[, "lower"]))` e R\$ `r brformat(exp(p3[, "higher"]))`.

# Referências {-}