\documentclass[a4paper, 12pt]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[left=3.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{hyperref}
\PassOptionsToPackage{usenames,dvipsnames}{color} % color is loaded by hyperref
\hypersetup{unicode=true,
            pdftitle={Regressão Quantílica},
            pdfauthor={Luiz Fernando Palin Droubi; Carlos Augusto Zilli; Murilo Damian Ribeiro; Norberto Hochheim},
            colorlinks=true,
            linkcolor=red,
            citecolor=blue,
            urlcolor=magenta,
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[style=abnt]{biblatex}

\addbibresource{bibliography.bib}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Regressão Quantílica}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
  \subtitle{Quando utilizar!?}
  \author{Luiz Fernando Palin Droubi\footnote{SPU/SC,
  \href{mailto:lfpdroubi@gmail.com}{\nolinkurl{lfpdroubi@gmail.com}}} \\ Carlos Augusto Zilli\footnote{IFSC,
  \href{mailto:carloszilli@gmail.com}{\nolinkurl{carloszilli@gmail.com}}} \\ Murilo Damian Ribeiro\footnote{UFSC,
  \href{mailto:murilodamianr@gmail.com}{\nolinkurl{murilodamianr@gmail.com}}
} \\ Norberto Hochheim\footnote{UFSC,
  \href{mailto:hochheim@gmail.com}{\nolinkurl{hochheim@gmail.com}}}}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{16/12/2019}

\usepackage{expl3}
\expandafter\def\csname ver@l3regex.sty\endcsname{} 
\usepackage{wrapfig}
\usepackage[brazil]{babel}
% \usepackage{natbib}
\usepackage{hyperref}
\usepackage{breakurl}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfig}
\usepackage{caption}
\usepackage{lastpage}
\setlength{\parindent}{1.25cm} % Default is 15pt.
\usepackage{indentfirst}
% \usepackage{helvet}
% \renewcommand{\familydefault}{\sfdefault}
% \usepackage{newtxtext,newtxmath} % mais nova para Times.
\usepackage{mathptmx} % para Times New Roman (preferível newtxtext)
% \usepackage{Times} % para Times New Roman (obsoleto)
% \usepackage{fontspec} % para Arial e/ou Times (xelatex ou luatex)
% \setmainfont{Arial}
% \newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
% \let\proglang=\textsf
% \let\code=\texttt
\newcommand{\bcenter}{\begin{center}}
\newcommand{\ecenter}{\end{center}}
\usepackage{fancyhdr}
% Turn on the style
\pagestyle{fancy}
% Clear the header and footer
\fancyhead{}
\fancyfoot{}
% Set the right side of the footer to be the page number
\fancyfoot[R]{\thepage~/~\pageref{LastPage}}

\begin{document}
\maketitle

\hypertarget{resumo}{%
\section*{Resumo}\label{resumo}}
\addcontentsline{toc}{section}{Resumo}

A NBR 14.653-02 recomenda que, na Engenharia de Avaliações de imóveis
urbanos, para o tratamento dos dados, seja utilizada metodologia
científica, mesmo no tratamento de dados por fatores, o que usualmente é
feito através do método da regressão linear, ainda que a norma também
cite outros métodos, como a regressão espacial, a análise envoltória de
dados e as redes neurais artificiais. No entanto, através destes
métodos, o que se obtém são coeficientes ou fatores \textbf{médios} da
contribuição de uma característica do imóvel na formação do valor final.
Ocorre que a contribuição de uma determinada característica para a
formação do valor final dos imóveis pode ser diferente para os
diferentes quantis da distribuição de probabilidades. É possível até que
uma determinada característica que se mostre insignificante no método da
regressão linear seja significativa na regressão quantílica, já que na
regressão linear o que se estima é se, \textbf{em média}, uma
determinada característica tem influência na formação do valor total de
um imóvel. Ocorre que uma determinada característica pode influenciar
positivamente o preço de venda dos imóveis de maior valor e
negativamente o preço de venda dos imóveis de menor valor (ou \emph{vice
versa}), sendo que, \textbf{em média}, o seu efeito seja nulo, o que no
entanto não quer dizer que aquela variável não tenha qualquer influência
na formação de preço dos imóveis do mercado em análise. Em suma, esta
diferente contribuição das características no valor final dos imóveis,
atualmente, é ignorada, sendo apenas utilizado o valor médio, sendo que
diferentes efeitos das características em imóveis de valores diferentes
são negligenciadas. A regressão quantílica é um método que permite
estimar a real influência de cada característica ao longo de toda a
distribuição de probabilidades dos imóveis de um mercado, o que pode se
demonstrar útil na avaliação de imóveis urbanos em determinados mercados
o que atualmente pode passar despercebido aos olhos do avaliador
acostumado com os métodos estatísticos clássicos.

\hypertarget{introducao}{%
\section{Introdução}\label{introducao}}

\textcite{Zietz} mostra que os conflitos a respeito da contribuição de
uma determinada característica na formação dos preços de venda de
imóveis residenciais podem ser esclarecidos através da regressão
quantílica. Diferentes valores para os coeficientes de regressão linear
para algumas características podem ser encontrados ao longo da
distribuição de preços de venda de imóveis. Ou seja, algumas
características dos imóveis residenciais podem ser mais valorizados por
compradores de imóveis de mais alto valor do que por compradores de
imóveis de menor valor.

Segundo \textcite{Zietz}, variáveis como área construída, área do lote e
número de banheiros tem um impacto maior nos imóveis de maior valor de
venda, enquanto outras variáveis parecem ter um comportamento constante
para todos os preços de venda de imóveis, como garagens e distância ao
centro, entre outras.

A regressão quantílica permite que a influência de uma característica
qualquer de um imóvel tenha efeitos diferentes para diferentes faixas de
valores de imóveis.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/-000} 

}

\caption{Mudanças na densidade de trutas ($y$) em função do quociente largura/profundidade de um canal ($X$). Fonte: \textcite[413]{QReco}.}\label{fig:trutas}
\end{figure}

É possível perceber em alguns casos como o mostrado na figura
\ref{fig:trutas}, que com a regressão linear pode-se chegar à conclusão
de que não há qualquer correlação entre duas variáveis, já que, em
média, o efeito do regressor é nulo (\(\beta = 0\)), o que pode ser
visto na reta tracejada na figura, que é a reta de regressão linear.

No entanto, como bem observaram \textcite[412-413]{QReco}, se os autores
somente tivessem se baseado na regressão linear, eles teriam
erroneamente concluído que não há relação alguma entre as variávies
\(X\) e \(y\). Na realidade, a relação pode ser percebida apenas nos
quantis superiores dos dados. Fisicamente, o que deve ser deduzido é
que, com o aumento do quociente largura/profundidade de um canal,
limita-se a densidade de trutas no canal. Esta limitação não é percebida
nos quantis inferiores dos dados. Por isso, em média, o efeito é nulo,
mas claramente o efeito pode ser percebido nos quantis superiores.

O mesmo comportamento poderia ser encontrado na Engenharia de
Avaliações. Imagine-se que ao analisar um mercado de lotes urbanos o
Engenheiro de Avaliações se depare com os dados mostrados na figura
\ref{fig:urb}. Ao analisar os dados através da regressão linear à média
(reta tracejada), o Engenheiro poderia, erroneamente concluir que a área
não teria nenhum efeito sobre o valor unitário dos lotes, quando na
realidade o que ocorre é que o efeito da variável área é o de aumentar
suavemente o valor unitário dos lotes de menor valor, e diminuir de uma
forma um pouco mais agressiva o valor unitário dos lotes dos quantis
superiores.

Em suma, globalmente, o efeito médio é zero, o que não significa que a
variável não possua qualquer significância na formação de valor dos
imóveis.

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/urb-1} 

}

\caption{Dados gerados randomicamente para ilustrar a importância da regressão quantílica na Engenharia de Avaliações. Fonte: do Autor.}\label{fig:urb}
\end{figure}

\hypertarget{revisao-bibliografica}{%
\section{Revisão Bibliográfica}\label{revisao-bibliografica}}

\hypertarget{breve-historico}{%
\subsection{Breve Histórico}\label{breve-historico}}

\textcite{stigler1986}

Segundo \textcite[p.~347]{koenker2000}, o gráfico mais importante da
história da estatística é o gráfico de Galton, reproduzido na figura 1.

O gráfico ilustra o fenômeno, descoberto por Galton, da regressão à
média, cuja importância até hoje se faz presente em diversos estudos
científicos. Em estudos clínicos para a aferição do real efeito de um
novo medicamento, por exemplo, há necessidade de se estabelecerem dois
grupos de pacientes (chamados de grupos de controle e de grupo de
tratamento) para isolar os efeitos do tratamento pesquisado do efeito do
regressão à média (ou reversão à média) \autocite[ver][]{james1973}.
Nestes estudos, apenas as pessoas do grupo de tratamento realmente são
tratadas com o mediamento em teste. Desta maneira, a diferença das
médias entre os grupos é isolada da variação biológica natural e da
variação devido aos erros de aferição, que estão sujeitos à regressão à
média.

\begin{figure}
\centering
\includegraphics{image_Galton.png}
\caption{Regressão à média (Galton, 1885). Fonte:
\textcite[348]{koenker2000}}
\end{figure}

Segundo \textcite[p.~349]{koenker2000}, a característica essencial da
regressão linear clássica, derivada deste gráfico, é que o efeito do
covariante na variável resposta é inteiramente capturado pela expressão
abaixo, uma simples ``mudança de local'', enquanto a aleatoriedade
remanescente de \(Y\) dado \(X\) é modelada por um termo de erro aditivo
independente de \(X\).

\[\mathbb{E}(Y|X = x) = x'\beta\]

Talvez prevendo que o seu método fizesse os seus colegas estatísticos se
aterem apenas ao estudo das médias, \textcite[p.~62]{galton}
\autocite[\emph{apud}][p.~350]{koenker2000} repreendeu seus colegas
estatísticos que:

\begin{quote}
limited their inquiries to Averages, and do not seem to revel in more
comprehensive views. Their souls seem as dull to the charm of variety as
that of a native of one of our flat English counties, whose retrospect
of Switzerland was that, if the mountains could be thrown into its
lakes, two nuisances would be got rid of at once.
\end{quote}

Ironicamente, contudo, apesar da repreensão de Galton, muito
provavelmente foi o seu método o principal responsável por restringir o
escopo das investigações estatísticas à comparação de médias por décadas
\autocite[350]{koenker2000}.

Muito anterior ao descobrimento por \textcite{galton} do fenômeno da
regressão à média e ainda anterior ao descobrimento do método dos
mínimos quadrados por \textcite{legendre1805} \footnote{\textcite{gauss1809}
  ligou o método dos mínimos quadrados à distribuição normal mas a
  origem do método se deve ao trabalho pioneiro de
  \textcite{legendre1805}. Houve discordâncias entre os dois na disputa
  pela invenção do método, entre outros achados na época. Ver a este
  respeito \textcite{STIGLER197731}; \textcite{stigler1981} e
  \textcite{stigler1986}.}, Boscovich propôs, em 1760, o seguinte
problema, em busca de estimar se a hipótese elipsoidal, prevista na
obra-prima de \textcite{newton}, estaria correta, em função de cinco
observações do comprimento de um grau de latitude obtidas em diferentes
longitudes
\autocites[p.~353]{koenker2000}[p.~281]{tortoise}[p.~40]{stigler1986}:

Encontrar \(\hat \alpha\) e \(\hat \beta\) tais que:

\[y_i = \hat \alpha + \hat \beta x_i + \hat u_i\]

com \(\sum \hat u_i = 0\) e \(\sum |\hat u_i| = \text{min!}\).

\textcite{boscovich} propôs uma solução através de um algoritmo
geométrico. Alguns anos depois, \textcite{laplace1789} resolveu o
problema matematicamente em 1789 \autocite[281]{tortoise}, no que pode
ser considerada a primeira análise de regressão.

Posteriormente, com a chegada do \emph{méthode des moindres carrés}, ou
seja, do método dos mínimos quadrados ordinários, o método dos mínimos
erros absolutos de Boscovich/Laplace ficou em segundo plano. Até que
\textcite{edgeworth}, relaxando a restrição de que a soma dos resíduos
seja igual a zero (\(\sum \hat u_i = 0\)) \autocite[281]{tortoise},
propõe o primeiro o protótipo do que viria se tornar, na década de 1940,
o algoritmo simplex, capaz de obter, de forma iterativa, o intercepto e
o coeficiente angular da reta do método dos mínimos desvios absolutos.

Na década de 40 surgiram os primeiros algoritmos simplex destinados à
otimização, algoritmos estes que se ajustam às necessidades do métodos
dos mínimos desvios absolutos, que não possui solução analítica, mas
iterativa. A primeira aplicação do método dos mínimos desvios absolutos
se deve a Arrow e Hoffenberg, em 1959 \autocite[281]{tortoise}.

\textcite{barrodale} propuseram a forma moderna do algoritmo simplex
que, por muitos anos e até hoje é utilizado para a minimização do erro
médio absoluto.

\textcite{koenker1978} generalizaram o problema de minimização do erro
médio absoluto, o que equivale à regressão à mediana, ao problema de
encontrar os diversos quantis de distribuição através da aplicação de
uma função de perda assimétrica, correspondente àquele quantil,
chegando-se assim à regressão quantílica.

\hypertarget{referencial-teorico}{%
\subsection{Referencial teórico}\label{referencial-teorico}}

\hypertarget{interpretacao-geometrica}{%
\subsubsection{Interpretação
Geométrica}\label{interpretacao-geometrica}}

\hypertarget{o-grafico-dual}{%
\paragraph{O gráfico dual}\label{o-grafico-dual}}

\hypertarget{estimacao-de-quantis}{%
\subsubsection{Estimação de quantis}\label{estimacao-de-quantis}}

Existem diversas formas de se obter os quantis de uma amostra.

\hypertarget{o-problema-de-estimar-quantis-como-um-problema-de-minimizacao}{%
\subsubsection{O problema de estimar quantis como um problema de
minimização}\label{o-problema-de-estimar-quantis-como-um-problema-de-minimizacao}}

Pode-se demonstrar que, assim como a média aritmética \(\mu\) de uma
variável aleatória tem a propriedade de minimizar a soma dos desvios
quadráticos de cada observação em relação a ela
\autocite[50]{matloff2017}, a mediana tem a propriedade de minimizar a
soma dos desvios médios absolutos de cada observação
\autocite[260]{matloff2017}. Ou seja:

\[\mu(Y) = \mathbb{E}Y = \arg \min_c \sum_{i = 1}^n \frac{1}{n}(y_i - c)^2\]
\[Me = \arg \min_c \sum_{i = 1}^n \frac{1}{n}|y_i - c|\] Sabe-se que a
mediana de uma variável equivale ao quantil de 50\%. Assim, outros
quantis podem ser obtidos com formulação análoga à formulação acima,
porém com a aplicação de uma função de perda assimétrica
(\(\rho_\tau(.)\)) em lugar da função módulo (ver figura 1):

\[Q_\tau(Y) = \arg \min_c \sum_{i = 1}^n \rho_\tau(y_i - c)\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{DmKq7} 

}

\caption{Função de perda ou custo.}\label{fig:unnamed-chunk-1}
\end{figure}

\bcenter Fonte: \textcite{qr}. \ecenter

\hypertarget{regressao-linear-e-quantilica}{%
\subsubsection{Regressão linear e
quantílica}\label{regressao-linear-e-quantilica}}

A regressão linear pode ser vista como uma forma de minimização, assim
como a média de uma população pode ser visto como o problema de
minimização descrito acima.

A diferença é que no caso da regressão linear, ao invés de minimizar em
relação a um escalar, desta vez se minimiza o erro em prever uma
variável Y em relação a uma função de outra variável X, f(X). Pode-se
demonstrar que entre todas as funções f(X), a que minimiza o erro médio
quadrático de Y dado X (\(\mathbb{E}[(Y - f(X))^2]\)) é a função de
regressão \(\mu(t) = \mathbb{E}(Y|X=t)\) \autocite[49-50]{matloff2017}.

Analogamente, pode-se demonstrar que a mediana condicional é a função
que minimiza o erro médio absoluto de Y dado X
(\(\mathbb{E}(|Y-f(X)|)\)) \autocite[260-261]{matloff2017}.

\hypertarget{unicidade-da-solucao}{%
\paragraph{Unicidade da solução}\label{unicidade-da-solucao}}

Pode-se demonstrar que a regressão linear, ou seja, a minimização de
\(\mathbb{E}[(Y - X\beta)^2]\) possui uma única solução e esta solução
pode ser encontrada analiticamente, bastando para isso efetuar a
derivação parcial deste termo em relação a \(\beta\) e igualando-o a
zero, obtendo-se assim uma única solução para \(\beta\) a qual
usualmente designa-se \(\hat \beta\) \autocite[ver][49-50]{matloff2017}.

O mesmo não se pode dizer da regressão à mediana e, mais genericamente,
da regressão quantílica. Nesta abordagem, há múltiplas soluções
possíveis, assim como numa amostra de tamanho par existem duas medianas
possíveis. Ainda, as soluções do problema de minimização da regressão
quantílica não podem ser encontradas analiticamente, sendo necessária a
utilização de precessos iterativos para a obtenção do(s) mínimo(s).

Contudo, deve-se ter em mente que, em ambos os processos de minimização,
seja para a regressão linear ou para a regressão quantílica, trabalha-se
com apenas uma amostra da população estudada. Desta forma, os valores de
\(\hat \beta\) encontrados são apenas estimativas dos valores reais de
\(\beta\), ou seja, os valores da população.

A diferença entre as múltiplas soluções da regressão quantílica é da
ordem de \(1/n\), enquanto a amplitude da precisão da estimativa é de
tamanho \(1/\sqrt{n}\). Assim, presume-se que as múltiplas soluções
possíveis, para os casos práticos estão dentro da margem de erro para a
primeira estimativa encontrada pelo algoritmo.

\hypertarget{robustez-da-solucao}{%
\paragraph{Robustez da solução}\label{robustez-da-solucao}}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Presença de \emph{Outliers}
\end{enumerate}

\hypertarget{transformacao-e-retransformacao}{%
\paragraph{Transformação e
retransformação}\label{transformacao-e-retransformacao}}

Na regressão linear, com a aplicação do método à uma variável produto da
transformação da variável dependente original, não é suficiente a
simples retransformação dessa variável à escala original para a obtenção
da solução, já que \(f(\mathbb{E}(X)) \leq \mathbb{E}(f(X))\), devido à
desigualdade de Jensen \autocite[ver][p.~207]{moda_media_mediana}.

Já na regressão quantílica, com a aplicação de \textbf{transformações
monotônicas}, pode-se afirmar que o quantil da variável transformada
corresponde ao mesmo quantil da variável original. Assim, seja \(f(.)\)
uma transformação monotônica qualquer em \(\mathbb{R}\), e \(Q(.)\) a
função quantil, para uma variável aleatória qualquer \(Y\) pode-se
escrever:

\[Q_{f(Y)}(\tau) = f(Q_Y(\tau))\] \textcite[p.~39-40]{koenker1978}
elencam ainda quatro propriedades de equivariância para a regressão
quantílica, reproduzidas abaixo:

\[
\begin{aligned}
\hat \beta(\tau;\lambda y,X) &= \lambda \hat \beta(\tau; y, X), & \lambda \in[0,\infty), \\
\hat \beta(1-\tau;\lambda y,X) &= \lambda \hat \beta(\tau; y, X) & \lambda \in (-\infty,0],\\
\hat \beta(\tau;y + X\gamma,X)& = \hat \beta(\tau; y, X) + \gamma & \gamma \in \mathbb{R}^k,\\
\hat \beta(\tau;y,XA) &= A^{-1} \hat \beta(\tau; y, X) & A_{K \times K} \text{não-singular.}
\end{aligned}
\]

\hypertarget{eficiencia-computacional}{%
\paragraph{Eficiência computacional}\label{eficiencia-computacional}}

\hypertarget{eficiencia-estatistica}{%
\paragraph{Eficiência estatística}\label{eficiencia-estatistica}}

Pode-se demonstrar que, se por um lado a média amostral tem eficiência
estatística assintótica igual a 1 quando a distribuição dos dados é
normal ou gaussiana, ela ela tem menos da metade da eficiência da
mediana quando a distribuição for a distribuição de Laplace e tem
eficiência zero para a distribuição de Cauchy
\autocite[p.36]{koenker1978}.

Desta maneira, quando há incerteza quanto à real distribuição dos dados,
a média, no caso escalar, ou a média condicional (regressão linear),
considerada o estimador ótimo para o caso gaussiano, pode ser preterível
à mediana, ou outros estimadores, ditos ``ineficientes''
\autocite[p.36]{koenker1978}

\hypertarget{estimador-de-maxima-verossimilhanca}{%
\subparagraph{Estimador de máxima
verossimilhança}\label{estimador-de-maxima-verossimilhanca}}

Pode-se demonstrar que, quando a distribuição é normal o estimador de
máxima verossimilhança para o parâmetro \(\mu\) da distribuição é a
média amostral.

Analogamente, se a distribuição dos dados for a distribuição de Laplace,
o estimador de máxima verossimilhança para o parâmetro é a mediana.

Isto implica que, se a distribuição dos dados é normal, são necessários
\(\pi/2\) (1,57) vezes mais dados para que a estimativa de \(\mu\)
através da mediana seja tão eficiente quanto a estimativa através da
média. Isto implica, por sua vez, que intervalos de confiança obtidos
para a regressão quantílica são 25\% mais largos do que os intervalos de
confiança para a regressão linear
\autocites[354]{koenker2000}[92]{dasGupta}.

No entanto, se a distribuição dos dados for a distribuição de Laplace,
pode-se demonstrar que são necessários duas vezes mais dados para que a
média estime \(\mu\) com a mesma precisão da mediana.

\twocolumn

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/dist_normal-1} 

}

\caption{Distribuição Normal.}\label{fig:dist_normal}
\end{figure}

\[
\begin{aligned}
\hat \mu = \frac{1}{n}\sum x_i \\
\hat \sigma = \frac{1}{n-1} \sum_{i=1}^n (x_i - \hat \mu)^2\\
f(x|\mu, \sigma) = \frac{1}{\sigma\sqrt{2/\pi}}\exp \left (-\frac{1}{2}\frac{(x - \mu)^2}{\sigma^2} \right )
\end{aligned}
\]

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/dist_Laplace-1} 

}

\caption{Distribuição de Laplace.}\label{fig:dist_Laplace}
\end{figure}

\[
\begin{aligned}
\hat \mu = \arg\min_c \sum |x_i - c| \\
\hat \lambda = \frac{1}{n} \sum_{i=1}^n |x_i - \hat \mu| \\
f(x|\mu, \lambda) = \frac{1}{2 \lambda} \exp \left ( -\frac{|x - \mu|}{\lambda}\right )
\end{aligned}
\] \onecolumn

\hypertarget{inferencia}{%
\subsubsection{Inferência}\label{inferencia}}

\hypertarget{aplicacoes-da-regressao-quantilica}{%
\subsection{Aplicações da regressão
quantílica}\label{aplicacoes-da-regressao-quantilica}}

\hypertarget{na-engenharia-de-avaliacoes}{%
\subsubsection{Na Engenharia de
Avaliações}\label{na-engenharia-de-avaliacoes}}

\textcite{Zietz} mostra\ldots{}

\hypertarget{estudos-de-caso}{%
\section{Estudos de Caso}\label{estudos-de-caso}}

Para os estudos de caso foram utilizados os dados disponíveis em
\textcite{hochheim}.

\hypertarget{duas-dimensoes}{%
\subsection{Duas dimensões}\label{duas-dimensoes}}

Assim como na regressão linear, é mais fácil aa compreensão da regressão
quantílica através de exemplos em duas dimensões, e depois generalizar
para \(n\) dimensões.

Seja primeiramente o caso de dados heteroscedásticos. A figura
\ref{fig:qr1} ilustra a aplicação da regressão quantílica e da regressão
linear para este caso. Na figura \ref{fig:qr1}, a reta vermelha é a reta
de regressão linear entre as variáveis. A área sombreada em cinza é o
intervalo de confiança para a regressão linear @80\%. As retas azuis são
as retas de regressão quantílica para os quantis 0,1; 0,2; 0,3; 0,4;
0,5; 0,6; 0,7; 0,8 e 0,9.

A regressão quantílica neste caso pode ser usada para demonstrar a não
validade dos intervalos de confiança (IC) e predição (IP) para a
regressão linear para este tipo de dados: como a variância da população
não é constante, mas aumenta com o aumento da área, as retas da
regressão quantílica se abrem. Como os intervalos de confiança e
predição na inferência clássica são calculados considerando-se que a
variância da população é constante, este efeito não se observa no
formato do IC.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/qr1-1} 

}

\caption{Regressão Linear e Quantílica para dados heteroscedásticos.}\label{fig:qr1}
\end{figure}

Assim como na regressão linear, uma conveniente transformação das
variáveis pode ser aplicada para a obtenção da homoscedasticidade. Isto
pode ser visto na figura \ref{fig:qr2}, onde as retas para os diferentes
quantis obtidas pela regressão quantílica agora são praticamente
paralelas entre si, indicando que a heteroscedasticidade foi removida.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/qr2-1} 

}

\caption{Regressão Linear e Quantílica com dados transformados.}\label{fig:qr2}
\end{figure}

Os coeficientes das retas de regressão quantílica podem ser plotados
como na figura \ref{fig:coef1}. Nesta figura, a reta cheia vermelha
representa o coeficiente do modelo de regressão linear, enquanto a reta
preta pontilhada representa os vários coeficientes da regressão
quantílica. As retas vermelhas tracejadas representam o intervalo de
confiança de estimação do coeficiente de regressão linear. A área
sombreada em cinza representa os intervalos de confiança para os
coeficientes da regressão quantílica. Deve-se notar que, entre os
quantis aproximados de 0,3 e 0,55, os coeficientes da regressão
quantílica não são significamente diferentes, estatísticamente, do
coeficiente da regreessão linear.

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/coef1-1} 

}

\caption{Variação dos coeficientes de regressão quantílica (variáveis originais).}\label{fig:coef1}
\end{figure}

Já para os dados transformados, pode-se notar na figura \ref{fig:coef2}
que para todos os quantis, os coeficientes da regressão quantílica não
podem ser considerados estatisticamente diferentes do coeficiente da
regressão linear. Também se pode notar nesta figura como o estimador de
regressão linear, para uma variável normalmente distribuída e na
ausência de heteroscedasticidade, é mais eficiente do que o estimador da
regressão quantílica, como a teoria já prevê (ver
\textcite{matloff2017}, 238).

(Zilli, não sei se tu pesquisou isso na revisão bibliográfica, mas acho
que se não, era bom colocar! Colocar algo do tipo: as vantagens e
desvantagens da regressão quantílica. Apesar da regressão quantílica ser
robusta à presença de \emph{outliers}, ela é menos eficiente do que a
regressao linear, caso a distribuição da variável estudada seja normal,
claro.)

\begin{figure}[H]

{\centering \includegraphics[width=0.7\linewidth]{images/coef2-1} 

}

\caption{Variação dos coeficientes de regressão quantílica (variáveis transformadas).}\label{fig:coef2}
\end{figure}

\hypertarget{analise-multivariada}{%
\subsection{Análise Multivariada}\label{analise-multivariada}}

Para os dados obtidos de Hochheim \autocite*[22-23]{hochheim} foram
ajustados dois modelos, um de regressão linear, com os dados saneados, e
outro de regressão quantílica, utilizando-se a totalidade dos dados,
para os quantis 0,1; 0,2; 0,3; 0,4; 0,5; 0,6; 0,7; 0,8 e 0,9.

Na figura \ref{fig:coefs} podem ser vistos os valores dos coeficientes
de cada variável para os diferentes quantis. Pode-se perceber, mais uma
vez, que o valor dos coeficientes da regressão quantílica não diferem
significantemente dos coeficientes da regressão linear (exceção para
alguns quantis superiores nas variáveis \texttt{area\_total} e
\texttt{padrao}).

\begin{figure}[H]

{\centering \includegraphics[width=1\linewidth]{images/coefs-1} 

}

\caption{Coeficientes de regressão linear e quantílica. Análise multivariada.}\label{fig:coefs}
\end{figure}

Na tabela \ref{tab:fits} podem ser vistos os coeficientes e estatísticas
básicas dos modelos de regressão linear e de regressão à mediana
(quantil 0,5).

\begin{table}[!htbp] \centering 
  \caption{Comparação entre os modelos de regressão linear e regressão à mediana.} 
  \label{tab:fits} 
\begin{tabular}{@{\extracolsep{5pt}}lcc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{2}{c}{\textit{Dependent variable:}} \\ 
\cline{2-3} 
\\[-1.8ex] & \multicolumn{2}{c}{log(valor)} \\ 
\\[-1.8ex] & \textit{OLS} & \textit{quantile} \\ 
 & \textit{} & \textit{regression} \\ 
\\[-1.8ex] & (1) & (2)\\ 
\hline \\[-1.8ex] 
 area\_total & 0.001 & 0.002 \\ 
  & (0.001, 0.002) & (0.001, 0.003) \\ 
  & t = 5.113 & t = 2.300 \\ 
  & p = 0.00001$^{***}$ & p = 0.027$^{***}$ \\ 
  & & \\ 
 quartos & 0.164 & 0.162 \\ 
  & (0.118, 0.209) & (0.107, 0.217) \\ 
  & t = 4.626 & t = 3.788 \\ 
  & p = 0.00004$^{***}$ & p = 0.0005$^{***}$ \\ 
  & & \\ 
 suites & 0.061 & 0.080 \\ 
  & (0.018, 0.104) & (0.020, 0.139) \\ 
  & t = 1.810 & t = 1.712 \\ 
  & p = 0.078$^{***}$ & p = 0.095$^{***}$ \\ 
  & & \\ 
 garagens & 0.209 & 0.152 \\ 
  & (0.166, 0.252) & (0.075, 0.230) \\ 
  & t = 6.247 & t = 2.520 \\ 
  & p = 0.00000$^{***}$ & p = 0.016$^{***}$ \\ 
  & & \\ 
 log(dist\_b\_mar) & $-$0.141 & $-$0.146 \\ 
  & ($-$0.176, $-$0.106) & ($-$0.210, $-$0.081) \\ 
  & t = $-$5.174 & t = $-$2.904 \\ 
  & p = 0.00001$^{***}$ & p = 0.006$^{***}$ \\ 
  & & \\ 
 rec(padrao) & $-$0.563 & $-$0.459 \\ 
  & ($-$0.697, $-$0.428) & ($-$0.650, $-$0.267) \\ 
  & t = $-$5.360 & t = $-$3.070 \\ 
  & p = 0.00001$^{***}$ & p = 0.004$^{***}$ \\ 
  & & \\ 
 Constant & 13.564 & 13.574 \\ 
  & (13.268, 13.859) & (13.100, 14.047) \\ 
  & t = 58.847 & t = 36.732 \\ 
  & p = 0.000$^{***}$ & p = 0.000$^{***}$ \\ 
  & & \\ 
\hline \\[-1.8ex] 
Observations & 48 & 50 \\ 
R$^{2}$ & 0.956 &  \\ 
Adjusted R$^{2}$ & 0.950 &  \\ 
Residual Std. Error & 0.136 (df = 41) &  \\ 
F Statistic & 148.921$^{***}$ (df = 6; 41) &  \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{2}{r}{$^{*}$p$<$0.3; $^{**}$p$<$0.2; $^{***}$p$<$0.1} \\ 
\end{tabular} 
\end{table}

\hypertarget{estimativas}{%
\subsubsection{Estimativas}\label{estimativas}}

É interessante comparar as estimativas obtidas com os modelos de
regressão linear, com dados saneados, e o modelo de regressão à mediana,
com a totalidade dos dados. Por um lado, o modelo de regressão linear
tende a ser mais preciso para a estimação da média, como prevê a teoria.
Por outro lado, com mais dados, o modelo de regressão à mediana pode
tornar-se mais eficiente.

Deve-se levar em conta que as estimativas com o modelo de regressão
linear aqui apresentadas são para a mediana da distribuição lognormal.

Pelo modelo de regressão linear, o valor da estimativa central
encontrado foi de R\$961.660,64, com intervalo de confiança entre R\$
924.768,13 e R\$ 1.000.024,94. A amplitude do intervalo de confiança foi
de 7.83\%.

Já pelo modelo de regressão quantílica, o valor da estimativa central
encontrado foi de R\$946.467,87, com intervalo de confiança entre R\$
886.472,34 e R\$ 1.010.523,85. A amplitude do intervalo de confiança foi
de 13.1\%.

O modelo de regressão linear mostrou-se, portanto, mais eficiente do que
o modelo de regressão a mediana, apesar no menor número de dados.

Os limites inferior e superior do intervalo de predição @80\% para o
modelo de regressão linear são, respectivamente: R\$ 802.017,63 e R\$
1.153.080,88.

Para o modelo de regressão quantílica, o intervalo de predição não faz
qualquer sentido. No entanto, é possível estimar os valores diretamente
para os quantis 0,1 e 0,9 da população. Nesta caso, os valores
encontrados foram, respectivamente: R\$ 810.629,32 e R\$ 1.186.954,14.

Podem ainda ser calculados os intervalos de confiança @80\% para as
estimativas dos quantis 0,1 e 0,9.

Os limites inferior e superior do IC para o quantil 0,1 são,
respectivamente: R\$ 781.253,06 e R\$ 841.110,17.

Os limites inferior e superior do IC para o quantil 0,9 são,
respectivamente: R\$ 1.116.547,53 e R\$ 1.261.800,41.

\printbibliography[title=Referências]


\end{document}
